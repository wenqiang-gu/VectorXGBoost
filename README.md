# VectorXGBoost

## Overview
The **VectorXGBoost** model extends XGBoost by enabling it to process **scalar and vector (jagged array) features** efficiently. This model:
1. Trains **"Gen-1" XGBoost models** on different subsets of features (scalar or jagged array).
2. Uses the **Gen-1 models' predicted scores** as new features.
3. Trains a **final XGBoost classifier** using the **Gen-1 scores + remaining features**.

This approach helps capture feature interactions and **boosts model performance**, especially in structured data problems with mixed feature types.

---

## Installation
### 1. Clone the repository
```bash
git clone https://github.com/yourusername/vector-xgboost.git
cd vector-xgboost
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

Alternatively, install the package directly:
```bash
pip install .
```

---

## How It Works
### Step 1: Train Generation-1 XGBoost Models
- Each **sub-model** is trained on a **specific subset of features**.
- Uses **cross-validation (`cross_val_predict`)** to prevent data leakage.
- Produces **out-of-fold predictions** for use in the next stage.

### Step 2: Train Final XGBoost Model
- Uses:
  - **Gen-1 models' predicted scores**
  - **Remaining features** not used in Gen-1 models
- Trains a final **XGBoost classifier** for prediction.

---

## Usage and Running Tests
To see an example of how to **train, evaluate, and use VectorXGBoost**, refer to the script in `example/train_example.py`.

To ensure everything works correctly, run unit tests:
```bash
pytest tests/
```

This will check the implementation against sample datasets.

---

## Customization
VectorXGBoost offers several configurable parameters to tailor the model to your specific dataset and needs. Below are the key aspects you can modify:

### 1. Define Feature Groups
Specify which features should be processed by different Gen-1 models:
```python
gen1_feature_groups = {
    "sub_model1": [0, 1],  # Scalar features
    "sub_model2": [2, 3]   # Jagged array features
}
```
This allows different parts of your dataset to be handled separately before being merged into the final classifier.

### 2. Handle Jagged Features
If your dataset includes jagged (variable-length) array features, specify their indices:
```python
jagged_features = [2, 3]  # Indices of jagged features
```
VectorXGBoost will expand these features during training and apply aggregation (default: max pooling) when making predictions.

### 3. Adjust XGBoost Parameters
Fine-tune the Gen-1 and final XGBoost classifiers separately:
```python
gen1_params = {
    "n_estimators": 50,
    "max_depth": 3,
    "learning_rate": 0.1
}
gen2_params = {
    "n_estimators": 100,
    "max_depth": 4,
    "learning_rate": 0.05
}
```
These parameters control the complexity and learning behavior of the model.

### 4. Modify Cross-Validation Strategy
Control how out-of-fold predictions are generated by adjusting the number of folds:
```python
n_splits = 5  # Number of cross-validation folds
```
This affects how the Gen-1 models are trained and evaluated.

### 5. Custom Aggregation for Jagged Features
By default, VectorXGBoost takes the max score from expanded jagged features. If you prefer a different method (e.g., mean or median), modify the `_aggregate_jagged_scores()` function.

---

## License
This project is licensed under the MIT License.